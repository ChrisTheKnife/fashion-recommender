{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer  (distilbert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cudf\n",
    "#import cupy as cp\n",
    "# import nvtabular as nvt\n",
    "\n",
    "# from google.cloud import bigquery\n",
    "\n",
    "import pandas as pd\n",
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# client = Client(n_workers=4, threads_per_worker=4, memory_limit=\"1GB\")\n",
    "# client\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"../key/key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfromer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nmslib \n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert = SentenceTransformer('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/data_10000.pickel')\n",
    "# Add one \"0\" to allign the customer_id to it's appearance in the submission csv: \n",
    "df['article_id'] = df['article_id'].apply(lambda x: \"0\"+x)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = distilbert.encode(df['description'], convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "distilbert_index.addDataPointBatch(embeddings)\n",
    "distilbert_index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cloth(dataframe, userQuery):\n",
    "    \n",
    "    if dataframe is not None and userQuery is not None:\n",
    "        df = dataframe.copy()\n",
    "        query = distilbert.encode([userQuery], convert_to_tensor=True)\n",
    "        ids, distances = distilbert_index.knnQuery(query, k=12)\n",
    "\n",
    "    matches = [] \n",
    "   \n",
    "    for i, j in zip(ids, distances):          \n",
    "            matches.append({'article_id':df.article_id.values[i]\n",
    "                        ,\"article_count\":df.article_count.values[i]\n",
    "                        ,\"price\":df.Avg_price.values[i]\n",
    "                        , 'description':df.description.values[i]\n",
    "                        , 'detail_desc' : df.detail_desc.values[i]\n",
    "                        , 'distance': j\n",
    "                       })     \n",
    "    return pd.DataFrame(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cloth(df,\"0732413001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign recommendations to customer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction dataset:\n",
    "df_trans = pd.read_csv('../data/transactions_train.csv', dtype={'article_id':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split (transactions without the last 4 weeks):\n",
    "df_trans_train = df_trans.query('t_dat < \"2020-08-26\"').copy()\n",
    "# Drop not necessary columns:\n",
    "df_trans_red = df_trans_train.drop(columns=['t_dat', 'price', 'sales_channel_id']).copy()\n",
    "# Generate wardrobe:\n",
    "df_wardrobe = df_trans_red.groupby('customer_id')['article_id'].aggregate(lambda x: list(x)).reset_index()\n",
    "# Reverse wardrobe article_ids to focus on the last bought items:\n",
    "df_wardrobe['wardrobe_reverse'] = df_wardrobe['article_id'].apply(lambda x: list(reversed(x)))\n",
    "# Cut wardrobe_reverse to max 3 article_ids:\n",
    "df_wardrobe['wardrobe_max_3'] = df_wardrobe['wardrobe_reverse'].apply(lambda x: x[:3] if len(x) > 3 else x)\n",
    "# Generate empty column for recommendation\n",
    "df_wardrobe['reco'] = np.nan\n",
    "# Drop not necessary columns\n",
    "df_wardrobe.drop(columns=['article_id', 'wardrobe_reverse'], inplace=True)\n",
    "\n",
    "df_wardrobe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del [[df_trans,df_trans_red, df_trans_train]]\n",
    "import gc\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed search function in order to speed up process in the recommendation-assignment:\n",
    "\n",
    "def search_cloth_new(dataframe, userQuery):\n",
    "    \n",
    "    if dataframe is not None and userQuery is not None:\n",
    "        df = dataframe.copy()\n",
    "        query = distilbert.encode([userQuery], convert_to_tensor=True)\n",
    "        ids, distances = distilbert_index.knnQuery(query, k=12)\n",
    "\n",
    "    matches = [] \n",
    "   \n",
    "    for i, j in zip(ids, distances):          \n",
    "            matches.append({'article_id':df.article_id.values[i]\n",
    "                        , 'distance': j\n",
    "                       })     \n",
    "    return pd.DataFrame(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cloth_new(df,\"0706016001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear reco-column, if recommendations should be re-build:\n",
    "# df_wardrobe['reco'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample to speed up process:\n",
    "df_sample = df_wardrobe.sample(n=200000, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations for each customer_id in dataframe:\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over wardrobes:\n",
    "for i, element in enumerate(tqdm(df_sample.wardrobe_max_3)):\n",
    "    no_to_reco = int(12/len(element))\n",
    "    recos = []\n",
    "    for j, item in enumerate(element):\n",
    "        # Store articles to recommend based on distance on certian article_id:\n",
    "        reco_art = search_cloth_new(df, item)\n",
    "        # Store recommendations per article_id in list:\n",
    "        reco_on_wr = reco_art.nsmallest(no_to_reco, 'distance').article_id.to_list()\n",
    "        # Append the recommendations to one list:\n",
    "        recos.append(reco_on_wr)\n",
    "    # Flatten the recos-\"list of lists\":\n",
    "    recos = [item for sublist in recos for item in sublist]\n",
    "    # Store recommendations in reco-column:\n",
    "    df_sample['reco'].iloc[i] = recos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('../data/20220510_Transformer_Recos_2000000_wardrobesize_1-3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reco-column (\"prediction\") which stores recos in one string:\n",
    "df_sample['prediction'] = df_sample.reco.apply(lambda x: ' '.join(x))\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission train file (only customer_ids which exist in train dataset):\n",
    "df_submission_train = df_sample.drop(columns=['wardrobe_max_3', 'reco'])\n",
    "df_submission_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample_submission to get all customer_ids and drop sample predictions:\n",
    "df_sample_sub = pd.read_csv('../data/baseline_sample_submission.csv')\n",
    "# Drop all rows where customer_ids have no recommendation:\n",
    "df_sample_sub.drop(columns=['prediction'], inplace=True)\n",
    "df_sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full submission file with all customer_ids:\n",
    "df_submission_test = pd.merge(df_sample_sub, df_submission_train, how= 'left', on=\"customer_id\")\n",
    "# df_submission_test_100000 = df_submission_test.dropna(axis=0).copy()\n",
    "df_submission_test_200000 = df_submission_test.copy()\n",
    "df_submission_test_200000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer without baseline and 200000 customer_ids: Store submission file as csv:\n",
    "df_submission_test_200000.to_csv('../data/Transformer-wo-Baseline-200000_20220510_submission_wardrobesize_1-3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission csv with baseline fillup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill up with baseline-model recommendations:\n",
    "\n",
    "df_submission_test_200000_baseline = df_submission_test.copy()\n",
    "baseline = \"0706016001 0706016002 0372860001 0610776002 0759871002 0464297007 0372860002 0610776001 0399223001 0706016003 0720125001 0156231001\"\n",
    "df_submission_test_200000_baseline['prediction'].fillna(value=baseline, inplace=True)\n",
    "df_submission_test_200000_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 50000 customer_ids and others filled up with baseline recommendations: Store submission file as csv:\n",
    "df_submission_test_200000_baseline.to_csv('../data/Transformer-Baseline-200000_20220510_submission_wardrobesize_1-3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.0 BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over wardrobes:\n",
    "# for i in tqdm(range(len(df_wardrobe))):\n",
    "# # for i in tqdm(range(10000)):\n",
    "#     # Calculate and store how much articles should be recommended for each ingoing article_id:\n",
    "#     no_to_reco = int(12/len(df_wardrobe.wardrobe_max_3.iloc[i]))\n",
    "#     recos = []\n",
    "#     for j in range(len(df_wardrobe.wardrobe_max_3.iloc[i])):\n",
    "#         # Store articles to recommend based on distance on certian article_id:\n",
    "#         reco_art = search_cloth(df, df_wardrobe.wardrobe_max_3.iloc[i][j])\n",
    "#         # Store recommendations per article_id in list:\n",
    "#         reco_on_wr = reco_art.nsmallest(no_to_reco, 'distance').article_id.to_list()\n",
    "#         # Append the recommendations to one list:\n",
    "#         recos.append(reco_on_wr)\n",
    "#     # Flatten the list of lists:\n",
    "#     recos = [item for sublist in recos for item in sublist]\n",
    "#     # Store recommendations in reco-column in wardrobe:\n",
    "#     df_wardrobe['reco'].iloc[i] = recos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD: only to test code\n",
    "\n",
    "# # for i in tqdm(range(len(df_wardrobe))):\n",
    "# for i in tqdm(range(5)):\n",
    "#     recos = []\n",
    "#     for j in range(len(df_wardrobe.wardrobe_max_3.iloc[i])):\n",
    "#         # search_art = df_wardrobe.wardrobe_max_3.iloc[i][j]\n",
    "#         reco_art = search_cloth(df, df_wardrobe.wardrobe_max_3.iloc[i][j])\n",
    "#         if len(df_wardrobe.wardrobe_max_3.iloc[i]) == 3:\n",
    "#             reco_on_3 = reco_art.nsmallest(4, 'distance').article_id.to_list()\n",
    "#             recos.append(reco_on_3)\n",
    "#             print(reco_on_3)\n",
    "#         if len(df_wardrobe.wardrobe_max_3.iloc[i]) == 2:\n",
    "#             reco_on_2 = reco_art.nsmallest(6, 'distance').article_id.to_list()\n",
    "#             recos.append(reco_on_2)\n",
    "#             print(reco_on_2)\n",
    "#         if len(df_wardrobe.wardrobe_max_3.iloc[i]) == 1:\n",
    "#             reco_on_1 = reco_art.nsmallest(12, 'distance').article_id.to_list()\n",
    "#             recos.append(reco_on_1)\n",
    "#             print(reco_on_1)\n",
    "#     recos = [item for sublist in recos for item in sublist]\n",
    "#     df_wardrobe['reco'].iloc[i] = recos\n",
    "#     print(f'recolist is: {recos}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6dc839308f78902ab8919870d8b0efc1a24d6ce128a31abf7bd1f309bd1934dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
